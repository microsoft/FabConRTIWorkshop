{"nbformat":4,"nbformat_minor":5,"metadata":{"language_info":{"name":"python"},"kernel_info":{"name":"synapse_pyspark","jupyter_kernel_name":null},"a365ComputeOptions":null,"sessionKeepAliveTimeout":0,"dependencies":{"lakehouse":{"default_lakehouse":"e31a201d-dc31-4173-97a4-795961a05afd","default_lakehouse_name":"LH_YCEcommLakehouse","default_lakehouse_workspace_id":"673d3828-dc48-4e03-a275-d64050e000a4","known_lakehouses":[{"id":"e31a201d-dc31-4173-97a4-795961a05afd"}]},"environment":{"environmentId":"5f2c9a37-04d0-4fcd-85db-b80ad835c742","workspaceId":"673d3828-dc48-4e03-a275-d64050e000a4"}}},"cells":[{"cell_type":"code","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"source":["from pyspark.sql import SparkSession\n","from delta.tables import DeltaTable"],"outputs":[]},{"cell_type":"code","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"source":["# Create Spark session\n","spark = SparkSession.builder.appName(\"CSV to Delta\").getOrCreate()"],"outputs":[]},{"cell_type":"code","metadata":{"tags":["parameters"],"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"source":["source_location = \"\"\n","relative_file_path = \"\""],"outputs":[]},{"cell_type":"code","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"source":["parts = source_location.split(\"/\")\n","tenant_id = parts[2]\n","workspace_id = parts[4]\n","workspace_id = workspace_id.replace(\" \", \"\")\n","lakehouse_id = parts[6]\n","lakehouse_id = lakehouse_id.replace(\" \", \"\")\n","relative_file_path = relative_file_path.replace(\" \", \"\")\n","\n","delta_table_path = \"Tables/shipmentevents\"\n"],"outputs":[]},{"cell_type":"code","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"source":["# ABFSS paths for input CSV and Delta output\n","csv_path = f\"abfss://{workspace_id}@onelake.dfs.fabric.microsoft.com/{lakehouse_id}/{relative_file_path}\"\n","delta_output_path = f\"abfss://{workspace_id}@onelake.dfs.fabric.microsoft.com/{lakehouse_id}/{delta_table_path}\""],"outputs":[]},{"cell_type":"code","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"source":["# Check if Delta table exists\n","table_exists = DeltaTable.isDeltaTable(spark, delta_output_path)\n","if table_exists:\n","    delta_table = DeltaTable.forPath(spark, delta_output_path)\n","    before_count = delta_table.toDF().count()\n","    print(f\"Rows in Delta table before load: {before_count}\")\n","else:\n","    before_count = 0\n","    print(\"Delta table does not exist. It will be created.\")\n","\n","# Read CSV with proper quote handling for embedded JSON\n","df = spark.read \\\n","    .option(\"header\", \"true\") \\\n","    .option(\"inferSchema\", \"true\") \\\n","    .option(\"quote\", \"\\\"\") \\\n","    .option(\"escape\", \"\\\"\") \\\n","    .csv(csv_path)\n","\n","csv_count = df.count()\n","print(f\"CSV rows read: {csv_count}\")\n","\n","# Write to Delta (overwrite mode)\n","df.write.format(\"delta\").mode(\"append\").save(delta_output_path)\n","print(\"CSV data loaded into Delta table.\")\n","\n","# Count rows after load\n","delta_table = DeltaTable.forPath(spark, delta_output_path)\n","after_count = delta_table.toDF().count()\n","print(f\"Rows in Delta table after load: {after_count}\")"],"outputs":[]}]}